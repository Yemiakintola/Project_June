# -*- coding: utf-8 -*-
"""streamlit_crop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hCGXjVDCR11C1F-s6bdibPM3g_lY0H66
"""

# app.py
import streamlit as st
import pandas as pd
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np

# Load the trained k-NN model
# Ensure the path to your model file is correct
try:
    with open('knn_model_optimized.pkl', 'rb') as f:
        model = pickle.load(f)
except FileNotFoundError:
    st.error("Error: Model file 'knn_model_optimized.pkl' not found. Please train and save the model first.")
    model = None

# Assuming you have access to the original data or the trained scaler and encoders
# for consistent preprocessing. For simplicity, we'll re-create a dummy scaler and encoder
# based on the structure of your processed data. In a real application, you should
# save and load your fitted scaler and one-hot encoder.

# Dummy data to fit scaler and encoder (should be based on your actual training data)
# Replace this with loading a small sample of your training data or saving
# the fitted scaler and one-hot encoder during your training phase.
dummy_data = {
    'Average_Rainfall': [100, 200, 300],
    'Average_Temperature': [15, 20, 25],
    'Average_Soil_pH': [6.0, 6.5, 7.0],
    'Soil_Texture': ['Clay', 'Loam', 'Sand'],
    'Planting_Season': ['Spring', 'Summer', 'Autumn']
}
dummy_df = pd.DataFrame(dummy_data)

# Fit a scaler and one-hot encoder on this dummy data
# IMPORTANT: In a production app, save and load the fitted scaler and encoder
# from your training pipeline.
scaler = StandardScaler()
scaler.fit(dummy_df[['Average_Rainfall', 'Average_Temperature', 'Average_Soil_pH']])

# Get unique categories for one-hot encoding based on your training data
# IMPORTANT: Replace with actual categories from your training data if available
soil_texture_categories = ['Clay', 'Loam', 'Sand', 'Silt', 'Peat', 'Chalk'] # Example categories
planting_season_categories = ['Spring', 'Summer', 'Autumn', 'Winter'] # Example categories

# Streamlit App Title
st.title('Crop Cluster Prediction')

st.write("""
Enter the environmental conditions and soil characteristics to predict the suitable crop cluster.
""")

# Input widgets
rainfall = st.number_input('Average Rainfall (mm)', min_value=0.0, value=150.0)
temperature = st.number_input('Average Temperature (Â°C)', min_value=-20.0, max_value=50.0, value=20.0)
soil_ph = st.number_input('Average Soil pH', min_value=0.0, max_value=14.0, value=6.5)
soil_texture = st.selectbox('Soil Texture', soil_texture_categories)
planting_season = st.selectbox('Planting Season', planting_season_categories)

# Prediction button
if st.button('Predict Crop Cluster'):
    if model is not None:
        # Prepare input data for prediction
        input_data = pd.DataFrame({
            'Average_Rainfall': [rainfall],
            'Average_Temperature': [temperature],
            'Average_Soil_pH': [soil_ph],
            'Soil_Texture': [soil_texture],
            'Planting_Season': [planting_season]
        })

        # One-hot encode categorical features
        # Use reindex to ensure all possible categories from training are present
        encoded_texture = pd.get_dummies(input_data['Soil_Texture'], prefix='Soil_Texture').reindex(columns=[f'Soil_Texture_{cat}' for cat in soil_texture_categories], fill_value=0)
        encoded_season = pd.get_dummies(input_data['Planting_Season'], prefix='Planting_Season').reindex(columns=[f'Planting_Season_{cat}' for cat in planting_season_categories], fill_value=0)

        # Handle potential NaNs from get_dummies(..., dummy_na=True) in training
        # Ensure columns match the training data after encoding
        # This requires knowing the exact columns from your df_processed
        # For simplicity here, we assume no NaNs were introduced in training dummy_na=False
        # If dummy_na=True was used, you need to handle Soil_Texture_nan and Planting_Season_nan
        # Example:
        # encoded_texture = pd.get_dummies(input_data['Soil_Texture'], prefix='Soil_Texture', dummy_na=True).reindex(columns=[f'Soil_Texture_{cat}' for cat in soil_texture_categories] + ['Soil_Texture_nan'], fill_value=0)
        # encoded_season = pd.get_dummies(input_data['Planting_Season'], prefix='Planting_Season', dummy_na=True).reindex(columns=[f'Planting_Season_{cat}' for cat in planting_season_categories] + ['Planting_Season_nan'], fill_value=0)


        # Combine numerical and encoded features
        processed_input = pd.concat([
            input_data[['Average_Rainfall', 'Average_Temperature', 'Average_Soil_pH']],
            encoded_texture,
            encoded_season
        ], axis=1)

        # Scale numerical features using the fitted scaler
        processed_input[['Average_Rainfall', 'Average_Temperature', 'Average_Soil_pH']] = scaler.transform(processed_input[['Average_Rainfall', 'Average_Temperature', 'Average_Soil_pH']])

        # Ensure the column order matches the training data
        # This is crucial for the model to make correct predictions.
        # You need the exact list of columns from your df_processed
        # Example:
        # expected_columns = ['Average_Rainfall', 'Average_Temperature', 'Average_Soil_pH', 'Soil_Texture_Clay', 'Soil_Texture_Loam', ...]
        # processed_input = processed_input.reindex(columns=expected_columns, fill_value=0)

        # IMPORTANT: The column order must match the training data exactly.
        # The simplest way to ensure this is to save the list of columns from df_processed
        # during training and load it here.

        # For this example, let's assume the columns are in a specific order
        # Replace this with the actual column order from your df_processed
        # Example: Assuming Soil_Texture columns come before Planting_Season columns
        all_cols = ['Average_Rainfall', 'Average_Temperature', 'Average_Soil_pH'] + \
                   [f'Soil_Texture_{cat}' for cat in soil_texture_categories] + \
                   [f'Planting_Season_{cat}' for cat in planting_season_categories]

        # Reindex to ensure correct column order and handle missing columns
        processed_input = processed_input.reindex(columns=all_cols, fill_value=0)


        # Make prediction
        try:
            prediction = model.predict(processed_input)
            predicted_cluster = prediction[0]

            # Display the prediction
            st.success(f'Predicted Crop Cluster: Cluster {predicted_cluster}')

        except Exception as e:
            st.error(f"An error occurred during prediction: {e}")
            st.error("Please ensure the input data format and columns match the training data.")
    else:
        st.warning("Model not loaded. Cannot make a prediction.")

# How to run this Streamlit app:
# 1. Save the code above as a Python file (e.g., app.py) in the same directory as your 'knn_model_optimized.pkl' file.
# 2. Open your terminal or command prompt.
# 3. Navigate to the directory where you saved the files.
# 4. Run the command:  streamlit run app.py
# 5. Your web browser should open with the Streamlit application.
